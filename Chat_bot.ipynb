{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ade4939a",
      "metadata": {
        "id": "ade4939a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f12037",
      "metadata": {
        "id": "14f12037"
      },
      "outputs": [],
      "source": [
        "with open(\"result.json\") as read_file:\n",
        "    data = json.load(read_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c627be2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6c627be2",
        "outputId": "aca41f6d-5e83-4496-94dd-e13dcd67bebb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ü§®'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['messages'][39]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4da943b",
      "metadata": {
        "id": "a4da943b"
      },
      "outputs": [],
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a5de53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a8a5de53",
        "outputId": "d395af61-f10f-4209-9617-5e91c1c13b26"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'—ç—Ç–æ —à–∏—Ñ—Ä '"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "remove_emoji('—ç—Ç–æ —à–∏—Ñ—Ä ü§®')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8cb03d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8cb03d0",
        "outputId": "554deff6-158a-4bf9-fdbb-4e8b708fdfd9"
      },
      "outputs": [],
      "source": [
        "last_person = ''\n",
        "phrase = ''\n",
        "questions = []\n",
        "answers = []\n",
        "for message in data['messages']:\n",
        "    if type(message['text']) != str:\n",
        "        continue\n",
        "    else:\n",
        "        text = remove_emoji(message['text'])\n",
        "        if text == '':\n",
        "            continue\n",
        "    try:\n",
        "        if message['from'] == last_person:\n",
        "            phrase += \" \" + text\n",
        "        else:\n",
        "            if last_person == '–î–∞–Ω–∏—Å –ë–∞—Ç—ã—Ä—à–∏–Ω':\n",
        "                questions.append(phrase)\n",
        "            else:\n",
        "                if phrase != '':\n",
        "                    if (len(answers) > 0):\n",
        "                        answers[-1] += \" <END>\"\n",
        "                    answers.append('<START> ' + phrase)\n",
        "            print(last_person)\n",
        "            print(phrase)\n",
        "            phrase = text\n",
        "    except KeyError:\n",
        "        if message['actor'] == last_person:\n",
        "            phrase += \" \" + text\n",
        "        else:\n",
        "            if last_person == '–î–∞–Ω–∏—Å –ë–∞—Ç—ã—Ä—à–∏–Ω':\n",
        "                questions.append(phrase)\n",
        "            else:\n",
        "                if phrase != '':\n",
        "                    if (len(answers) > 0):\n",
        "                        answers[-1] += \" <END>\"\n",
        "                    answers.append('<START> ' + phrase)\n",
        "            print(last_person)\n",
        "            print(phrase)\n",
        "            phrase = text\n",
        "\n",
        "    try:\n",
        "        last_person = message['from']\n",
        "    except KeyError:\n",
        "        last_person = message['actor']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OlEsbUjsuJAZ",
      "metadata": {
        "id": "OlEsbUjsuJAZ"
      },
      "outputs": [],
      "source": [
        "questions_short = []\n",
        "for q in questions:\n",
        "    questions_short.append(q[:200])\n",
        "answers_short = []\n",
        "for a in answers:\n",
        "    answers_short.append(a[:200])\n",
        "questions = questions_short[:3500]\n",
        "answers = answers_short[:3500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6d6b671",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6d6b671",
        "outputId": "b3a6feda-e744-41e6-92cc-5f92630d19dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3500"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc1a2f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfc1a2f5",
        "outputId": "cd161c80-0ed7-461c-f73d-74512f92b2db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3500"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "foAHNpRHfhHc",
      "metadata": {
        "id": "foAHNpRHfhHc"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import pad_sequences\n",
        "from keras import utils\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80adb9c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80adb9c8",
        "outputId": "6d98e867-ced6-45b8-e7f2-08619b141ff0"
      },
      "outputs": [],
      "source": [
        "# –ü–æ–¥–∫–ª—é—á–∞–µ–º –∫–µ—Ä–∞—Å–æ–≤—Å–∫–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —Å–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∏–Ω–¥–µ–∫—Å–æ–≤ { display-mode: \"form\" }\n",
        "#vocabularySize = 1000 #30000\n",
        "tokenizer = Tokenizer(num_words=None) #num_words=vocabularySize,  filters='!‚Äì\"‚Äî#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n\\r¬´¬ª'\n",
        "tokenizer.fit_on_texts(questions + answers) # –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —Å–±–æ—Ä–∫–∏ —Å–ª–æ–≤–∞—Ä—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏\n",
        "vocabularyItems = list(tokenizer.word_index.items()) # —Å–ø–∏—Å–æ–∫ —Å c–æ–¥–µ—Ä–∂–∏–º—ã–º —Å–ª–æ–≤–∞—Ä—è\n",
        "vocabularySize = len(vocabularyItems)+1 # —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è\n",
        "print( '–§—Ä–∞–≥–º–µ–Ω—Ç —Å–ª–æ–≤–∞—Ä—è : {}'.format(vocabularyItems[:100]))\n",
        "print( '–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è : {}'.format(vocabularySize))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qrFoVBzof0wr",
      "metadata": {
        "id": "qrFoVBzof0wr"
      },
      "outputs": [],
      "source": [
        "#–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ (–≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –æ—Ç–≤–µ—Ç—ã) { display-mode: \"form\" }\n",
        "def prepareDataForNN(phrases, isQuestion = True):\n",
        "  tokenizedPhrases = tokenizer.texts_to_sequences(phrases) # —Ä–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–æ–≤/–æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
        "  maxLenPhrases = max([ len(x) for x in tokenizedPhrases]) # —É—Ç–æ—á–Ω—è–µ–º –¥–ª–∏–Ω—É —Å–∞–º–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.–æ—Ç–≤–µ—Ç–∞\n",
        "  # –î–µ–ª–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–¥–Ω–æ–π –¥–ª–∏–Ω—ã, –∑–∞–ø–æ–ª–Ω—è—è –Ω—É–ª—è–º–∏ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã\n",
        "  paddedPhrases = pad_sequences(tokenizedPhrases, maxlen=maxLenPhrases, padding='post')\n",
        "\n",
        "  # –ü—Ä–µ–¥–ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ö–æ–¥–∞ –≤ —Å–µ—Ç—å\n",
        "  encoded = np.array(paddedPhrases) # –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ numpy –º–∞—Å—Å–∏–≤\n",
        "  phraseType = \"–≤–æ–ø—Ä–æ—Å\"\n",
        "  if not isQuestion:\n",
        "    phraseType = \"–æ—Ç–≤–µ—Ç\"\n",
        "  print('–ü—Ä–∏–º–µ—Ä –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ ' + phraseType + '–∞ –Ω–∞ –≤—Ö–æ–¥ : {}'.format(phrases[100]))\n",
        "  print('–ü—Ä–∏–º–µ—Ä –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ ' + phraseType + '–∞ –Ω–∞ –≤—Ö–æ–¥ : {}'.format(encoded[100]))\n",
        "  print('–†–∞–∑–º–µ—Ä—ã –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–∞—Å—Å–∏–≤–∞ ' + phraseType + '–æ–≤ –Ω–∞ –≤—Ö–æ–¥ : {}'.format(encoded.shape))\n",
        "  print('–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ ' + phraseType + '–æ–≤ –Ω–∞ –≤—Ö–æ–¥ : {}'.format(maxLenPhrases))\n",
        "\n",
        "  return encoded, maxLenPhrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KQnVW5FyhBwX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQnVW5FyhBwX",
        "outputId": "d6f5adc3-b62a-4d28-a4f8-756b2945b139"
      },
      "outputs": [],
      "source": [
        "encoderForInput, maxLenQuestions = prepareDataForNN(questions, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eHMQZL4bhLOF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHMQZL4bhLOF",
        "outputId": "06112b0d-f12e-4d6e-aa5b-d60ae4c49b5a"
      },
      "outputs": [],
      "source": [
        "decoderForInput, maxLenAnswers = prepareDataForNN(answers, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jlq2zBj5h5Rg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlq2zBj5h5Rg",
        "outputId": "f6648bdf-5f62-46c3-afa9-feb28483a8e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers: 3500\n",
            "tokenizedAnswers: 3500\n",
            "paddedAnswers: 3500\n",
            "vocabularySize: 7837\n"
          ]
        }
      ],
      "source": [
        "#–†–∞—Å–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ(–æ—Ç–≤–µ—Ç—ã) { display-mode: \"form\" }\n",
        "print(\"Answers:\", len(answers))\n",
        "tokenizedAnswers = tokenizer.texts_to_sequences(answers) # —Ä–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
        "print(\"tokenizedAnswers:\", len(tokenizedAnswers))\n",
        "for i in range(len(tokenizedAnswers)) : # –¥–ª—è —Ä–∞–∑–±–∏—Ç—ã—Ö –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤\n",
        "  tokenizedAnswers[i] = tokenizedAnswers[i][1:] # –∏–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç —Ç–µ–≥–∞ &lt;START>\n",
        "# –î–µ–ª–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–¥–Ω–æ–π –¥–ª–∏–Ω—ã, –∑–∞–ø–æ–ª–Ω—è—è –Ω—É–ª—è–º–∏ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã\n",
        "paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers , padding='post')\n",
        "print(\"paddedAnswers:\", len(paddedAnswers))\n",
        "print(\"vocabularySize:\", vocabularySize)\n",
        "oneHotAnswers = utils.to_categorical(paddedAnswers, vocabularySize) # –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ one hot vector\n",
        "decoderForOutput = np.array(oneHotAnswers) # –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –≤–∏–¥–µ –º–∞—Å—Å–∏–≤–∞ numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y9mpM0v0yGwl",
      "metadata": {
        "id": "Y9mpM0v0yGwl"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense\n",
        "from keras import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.vis_utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ryT9BYgt2YQn",
      "metadata": {
        "id": "ryT9BYgt2YQn"
      },
      "outputs": [],
      "source": [
        "# –ü–µ—Ä–≤—ã–π –≤—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π, –∫–æ–¥–µ—Ä, –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π { display-mode: \"form\" }\n",
        "encoderInputs = Input(shape=(None , ), name = \"EncoderForInput\") # —Ä–∞–∑–º–µ—Ä—ã –Ω–∞ –≤—Ö–æ–¥–µ —Å–µ—Ç–∫–∏ (–∑–¥–µ—Å—å –±—É–¥–µ—Ç encoderForInput)\n",
        "# –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ —Å–ª–æ–π Embedding (–¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞—Ä—è, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å)\n",
        "encoderEmbedding = Embedding(vocabularySize, 200 , mask_zero=True, name = \"Encoder_Embedding\") (encoderInputs)\n",
        "# –ó–∞—Ç–µ–º –≤—ã—Ö–æ–¥ —Å Embedding –ø–æ–π–¥—ë—Ç –≤ LSTM —Å–ª–æ–π, –Ω–∞ –≤—ã—Ö–æ–¥–µ —É –∫–æ—Ç–æ—Ä–æ–≥–æ –±—É–¥–µ—Ç –¥–≤–∞ –≤–µ–∫—Ç–æ—Ä–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è - state_h , state_c\n",
        "# –í–µ–∫—Ç–æ—Ä–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è - state_h , state_c –∑–∞–¥–∞–¥—É—Ç—Å—è –≤ LSTM —Å–ª–æ–µ –¥–µ–∫–æ–¥–µ—Ä–∞ –≤ –±–ª–æ–∫–µ –Ω–∏–∂–µ\n",
        "encoderOutputs, state_h , state_c = LSTM(200, return_state=True, name = \"Encoder_LSTM\")(encoderEmbedding)\n",
        "encoderStates = [state_h, state_c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cf0ImiMSyXxY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "id": "Cf0ImiMSyXxY",
        "outputId": "75d902f9-9b34-4bf2-a40e-24772f3e25cc"
      },
      "outputs": [],
      "source": [
        "#–í—Ç–æ—Ä–æ–π –≤—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π, –¥–µ–∫–æ–¥–µ—Ä, –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π { display-mode: \"form\" }\n",
        "decoderInputs = Input(shape=(None, ), name = \"DecoderForInput\") # —Ä–∞–∑–º–µ—Ä—ã –Ω–∞ –≤—Ö–æ–¥–µ —Å–µ—Ç–∫–∏ (–∑–¥–µ—Å—å –±—É–¥–µ—Ç decoderForInput)\n",
        "# –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ —Å–ª–æ–π Embedding (–¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞—Ä—è, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å)\n",
        "# mask_zero=True - –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω—É–ª–µ–≤—ã–µ padding –ø—Ä–∏ –ø–µ—Ä–µ–¥–∞—á–µ –≤ LSTM. –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç –≤—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞ —Ç–∏–ø–∞: \"–£ –º–µ–Ω—è –≤—Å–µ —Ö–æ—Ä–æ—à–æ PAD PAD PAD PAD PAD PAD..\"\n",
        "decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True, name = \"Decoder_Embedding\") (decoderInputs)\n",
        "# –ó–∞—Ç–µ–º –≤—ã—Ö–æ–¥ —Å Embedding –ø–æ–π–¥—ë—Ç –≤ LSTM —Å–ª–æ–π, –∫–æ—Ç–æ—Ä–æ–º—É –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤–µ–∫—Ç–æ—Ä–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è - state_h , state_c\n",
        "decoderLSTM = LSTM(200, return_state=True, return_sequences=True, name = \"Decoder_LSTM\")\n",
        "decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)\n",
        "# –ò –æ—Ç LSTM'–∞ —Å–∏–≥–Ω–∞–ª decoderOutputs –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π —Å —Å–æ—Ñ—Ç–º–∞–∫—Å–æ–º –Ω–∞ –≤—ã—Ö–æ–¥–µ\n",
        "decoderDense = Dense(vocabularySize, activation='softmax')\n",
        "output = decoderDense (decoderOutputs)\n",
        "#–°–æ–±–∏—Ä–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –º–æ–¥–µ–ª—å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ { display-mode: \"form\" }\n",
        "model = Model([encoderInputs, decoderInputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "print(model.summary()) # –≤—ã–≤–µ–¥–µ–º –Ω–∞ —ç–∫—Ä–∞–Ω –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏\n",
        "plot_model(model, to_file='model.png') # –∏ –ø–æ—Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–µ–≤ –∏ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –Ω–∏–º–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s8PzcdTIzY16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8PzcdTIzY16",
        "outputId": "8ff81425-dc3d-427b-84d9-0e8186c6ffca"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–ø—É—Å—Ç–∏–º –æ–±—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–∏–º –º–æ–¥–µ–ª—å\n",
        "model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hhhJatfG0KNL",
      "metadata": {
        "id": "hhhJatfG0KNL"
      },
      "outputs": [],
      "source": [
        "model.save_weights('chat_weights_v4.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NXuiGNNtChuH",
      "metadata": {
        "id": "NXuiGNNtChuH"
      },
      "outputs": [],
      "source": [
        "model.load_weights('chat_weights_v4.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-O4u0tO7A44G",
      "metadata": {
        "id": "-O4u0tO7A44G"
      },
      "outputs": [],
      "source": [
        "def makeInferenceModels():\n",
        "  # –û–ø—Ä–µ–¥–µ–ª–∏–º –º–æ–¥–µ–ª—å –∫–æ–¥–µ—Ä–∞, –Ω–∞ –≤—Ö–æ–¥–µ –¥–∞–ª–µ–µ –±—É–¥—É—Ç –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã(encoderForInputs), –Ω–∞ –≤—ã—Ö–æ–¥–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è state_h, state_c\n",
        "  encoderModel = Model(encoderInputs, encoderStates)\n",
        "  decoderStateInput_h = Input(shape=(200 ,), name = 'decoderStateInput_h') # –æ–±–æ–∑–Ω–∞—á–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è —Å —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º state_h\n",
        "  decoderStateInput_c = Input(shape=(200 ,), name = 'decoderStateInput_c') # –æ–±–æ–∑–Ω–∞—á–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è —Å —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º state_c\n",
        "  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] # –≤–æ–∑—å–º–µ–º –æ–±–∞ inputs –≤–º–µ—Å—Ç–µ –∏ –∑–∞–ø–∏—à–µ–º –≤ decoderStatesInputs\n",
        "  # –ë–µ—Ä—ë–º –æ—Ç–≤–µ—Ç—ã, –ø—Ä–æ—à–µ–¥—à–∏–µ —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–∏–Ω–≥, –≤–º–µ—Å—Ç–µ —Å —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏ –∏ –ø–æ–¥–∞—ë–º LSTM c–ª–æ—é\n",
        "  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)\n",
        "  decoderStates = [state_h, state_c] # LSTM –¥–∞—Å—Ç –Ω–∞–º –Ω–æ–≤—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
        "  decoderOutputs = decoderDense(decoderOutputs) # –∏ –æ—Ç–≤–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –ø—Ä–æ–ø—É—Å—Ç–∏–º —á–µ—Ä–µ–∑ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π —Å —Å–æ—Ñ—Ç–º–∞–∫—Å–æ–º\n",
        "  # –û–ø—Ä–µ–¥–µ–ª–∏–º –º–æ–¥–µ–ª—å –¥–µ–∫–æ–¥–µ—Ä–∞, –Ω–∞ –≤—Ö–æ–¥–µ –¥–∞–ª–µ–µ –±—É–¥—É—Ç —Ä–∞—Å–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (decoderForInputs) –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
        "  # –Ω–∞ –≤—ã—Ö–æ–¥–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç –∏ –Ω–æ–≤—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
        "  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)\n",
        "\n",
        "  print(decoderModel.summary()) # –≤—ã–≤–µ–¥–µ–º –Ω–∞ —ç–∫—Ä–∞–Ω –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏\n",
        "  plot_model(decoderModel, to_file='decoderModel.png') # –∏ –ø–æ—Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–µ–≤ –∏ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –Ω–∏–º–∏\n",
        "  return encoderModel , decoderModel\n",
        "\n",
        "def strToTokens(sentence: str): # —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞ –≤—Ö–æ–¥ (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –≤–æ–ø—Ä–æ—Å–æ–º)\n",
        "  words = sentence.lower().split() # –ø—Ä–∏–≤–æ–¥–∏—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —Ä–∞–∑–±–∏—Ä–∞–µ—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
        "  tokensList = list() # –∑–¥–µ—Å—å –±—É–¥–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤/–∏–Ω–¥–µ–∫—Å–æ–≤\n",
        "  for word in words: # –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏\n",
        "    tokensList.append(tokenizer.word_index[word]) # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –∏–Ω–¥–µ–∫—Å –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫\n",
        "    # –§—É–Ω–∫—Ü–∏—è –≤–µ—Ä–Ω—ë—Ç –≤–æ–ø—Ä–æ—Å –≤ –≤–∏–¥–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –¥–ª–∏–Ω–æ–π —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –∏–∑ –Ω–∞—à–µ–π –±–∞–∑—ã –≤–æ–ø—Ä–æ—Å–æ–≤\n",
        "  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1HmRg1rrAOTS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1HmRg1rrAOTS",
        "outputId": "2cd792d6-0197-4ab6-d581-258bb3f1bab2"
      },
      "outputs": [],
      "source": [
        "#–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –º–æ–¥–µ–ª—å { display-mode: \"form\" }\n",
        "encModel , decModel = makeInferenceModels() # –∑–∞–ø—É—Å–∫–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–æ–¥–µ—Ä–∞ –∏ –¥–µ–∫–æ–¥–µ—Ä–∞\n",
        "for _ in range(10): # –∑–∞–¥–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤, –∏ –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –≤ —ç—Ç–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ:\n",
        "  # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç –∫–æ–¥–µ—Ä –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –≤–æ–ø—Ä–æ—Å–æ–º\n",
        "  statesValues = encModel.predict(strToTokens(input( '–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å : ' )))\n",
        "  # –°–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ —Ä–∞–∑–º–µ—Ä–æ–º (1, 1)\n",
        "  emptyTargetSeq = np.zeros((1, 1))\n",
        "  emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # –ø–æ–ª–æ–∂–∏–º –≤ –ø—É—Å—Ç—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ 'start' –≤ –≤–∏–¥–µ –∏–Ω–¥–µ–∫—Å–∞\n",
        "  stopCondition = False # –∑–∞–¥–∞–¥–∏–º —É—Å–ª–æ–≤–∏–µ, –ø—Ä–∏ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–∏ –∫–æ—Ç–æ—Ä–æ–≥–æ, –ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—á–µ—Ä–µ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞\n",
        "  decodedTranslation = '' # –∑–¥–µ—Å—å –±—É–¥–µ—Ç —Å–æ–±–∏—Ä–∞—Ç—å—Å—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–π –æ—Ç–≤–µ—Ç\n",
        "  while not stopCondition : # –ø–æ–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ —Å—Ç–æ–ø-—É—Å–ª–æ–≤–∏–µ\n",
        "    # –í –º–æ–¥–µ–ª—å –¥–µ–∫–æ–¥–µ—Ä–∞ –ø–æ–¥–∞–¥–∏–º –ø—É—Å—Ç—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ —Å–ª–æ–≤–æ–º 'start' –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–¥–µ—Ä–æ–º –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É.\n",
        "    # –¥–µ–∫–æ–¥–µ—Ä –∑–∞–º–µ–Ω–∏—Ç —Å–ª–æ–≤–æ 'start' –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–ª–æ–≤–æ–º –∏ –æ–±–Ω–æ–≤–∏—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
        "    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "\n",
        "    #argmax –ø—Ä–æ–±–µ–∂–∏—Ç –ø–æ –≤–µ–∫—Ç–æ—Ä—É decOutputs, –Ω–∞–π–¥–µ—Ç –º–∞–∫—Å.–∑–Ω–∞—á–µ–Ω–∏–µ, –∏ –≤–µ—Ä–Ω—ë—Ç –Ω–æ–º–µ—Ä –∏–Ω–¥–µ–∫—Å–∞ –ø–æ–¥ –∫–æ—Ç–æ—Ä—ã–º –æ–Ω–æ –ª–µ–∂–∏—Ç –≤ –º–∞—Å—Å–∏–≤–µ\n",
        "    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) # argmax –≤–æ–∑—å–º–µ–º –æ—Ç –æ—Å–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π x —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –ü–æ–ª—É—á–∏–ª–∏ –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞.\n",
        "    sampledWord = None # —Å–æ–∑–¥–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é, –≤ –∫–æ—Ç–æ—Ä—É—é –ø–æ–ª–æ–∂–∏–º —Å–ª–æ–≤–æ, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫\n",
        "    for word , index in tokenizer.word_index.items():\n",
        "      if sampledWordIndex == index: # –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–∞–∫–æ–º—É-—Ç–æ –∏–Ω–¥–µ–∫—Å—É –∏–∑ —Å–ª–æ–≤–∞—Ä—è\n",
        "        decodedTranslation += ' {}'.format(word) # —Å–ª–æ–≤–æ, –∏–¥—É—â–µ–µ –ø–æ–¥ —ç—Ç–∏–º –∏–Ω–¥–µ–∫—Å–æ–º –≤ —Å–ª–æ–≤–∞—Ä–µ, –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç\n",
        "        sampledWord = word # –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ —Ñ–∏–∫—Å–∏—Ä—É–µ–º –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é sampledWord\n",
        "\n",
        "    # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Å–ª–æ–≤–æ–º –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è 'end' –ª–∏–±–æ –µ—Å–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∑–∞–¥–∞–Ω–Ω—É—é –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞\n",
        "    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:\n",
        "      stopCondition = True # —Ç–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç–æ–ø-—É—Å–ª–æ–≤–∏–µ –∏ –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é\n",
        "    emptyTargetSeq = np.zeros((1, 1)) # —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤\n",
        "    emptyTargetSeq[0, 0] = sampledWordIndex # –∑–∞–Ω–æ—Å–∏–º —Ç—É–¥–∞ –∏–Ω–¥–µ–∫—Å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞\n",
        "    statesValues = [h, c] # –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–µ–∫–æ–¥–µ—Ä–æ–º\n",
        "    # –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ü–∏–∫–ª —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "\n",
        "  print(decodedTranslation) # –≤—ã–≤–æ–¥–∏–º –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä–æ–º"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
